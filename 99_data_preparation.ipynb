{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bed23723-f1ab-4de0-80e6-331fa4e62ecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook downloads the [M4 dataset](https://www.sciencedirect.com/science/article/pii/S0169207019301128) using an open source package: [`datasetsforecast`](https://github.com/Nixtla/datasetsforecast/tree/main/). The M4 dataset is a large and diverse collection of time series data used for benchmarking and evaluating the performance of forecasting methods. It is part of the [M-competition](https://forecasters.org/resources/time-series-data/) series, which are organized competitions aimed at comparing the accuracy and robustness of different forecasting methods.\n",
    "\n",
    "This notebook is run by other notebooks using `%run` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e791bdb3-de54-455e-a555-1dd358f85f51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Install package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d557aac9-dc22-4aeb-aea5-aeb92a167d0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install datasetsforecast==0.0.8 --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39dff352-76eb-4f49-b7f2-871839a2bc11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Set the logging level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "992ef0ae-6446-4547-a977-12f38d40f52d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "from datasetsforecast.m4 import M4\n",
    "import logging\n",
    "\n",
    "logger = spark._jvm.org.apache.log4j\n",
    "\n",
    "# Setting the logging level to ERROR for the \"py4j.java_gateway\" logger\n",
    "# This reduces the verbosity of the logs by only showing error messages\n",
    "logging.getLogger(\"py4j.java_gateway\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"py4j.clientserver\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21c8644d-9fc6-4d49-9276-fcd4419a3741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create a catalog and a database\n",
    "We create a catalog and a database (schema) to store the delta tables for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53c009e1-9487-4d80-816e-1bda1eaee719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a text widget for the catalog name input\n",
    "dbutils.widgets.text(\"catalog\", \"\")\n",
    "# Creating a text widget for the database (schema) name input\n",
    "dbutils.widgets.text(\"db\", \"\")\n",
    "# Creating a text widget for the number of time series to sample input\n",
    "dbutils.widgets.text(\"n\", \"\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")  # Name of the catalog we use to manage our assets\n",
    "db = dbutils.widgets.get(\"db\")  # Name of the schema we use to store assets\n",
    "n = int(dbutils.widgets.get(\"n\"))  # Number of time series to sample\n",
    "\n",
    "# Ensure the catalog exists, create it if it does not\n",
    "_ = spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "# Ensure the schema exists within the specified catalog, create it if it does not\n",
    "_ = spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c222a2a9-8d58-416c-8ecb-f2c0adc25596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Daily M4 data\n",
    "Below are some custom functions to convert the downloaded M4 time series into a daily format. The parameter `n` specifies the number of time series to sample for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0bac8b2-6283-4d82-a0b9-d46505517d25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_m4_daily():\n",
    "    # Load the M4 daily dataset\n",
    "    y_df, _, _ = M4.load(directory=str(pathlib.Path.home()), group=\"Daily\")\n",
    "    # Create a list of unique IDs for the time series we want to sample\n",
    "    _ids = [f\"D{i}\" for i in range(1, n)]\n",
    "    # Filter and transform the dataset based on the unique IDs\n",
    "    y_df = (\n",
    "        y_df.groupby(\"unique_id\")\n",
    "        .filter(lambda x: x.unique_id.iloc[0] in _ids)\n",
    "        .groupby(\"unique_id\")\n",
    "        .apply(transform_group_daily)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return y_df\n",
    "\n",
    "\n",
    "def transform_group_daily(df):\n",
    "    unique_id = df.unique_id.iloc[0]  # Get the unique ID of the current group\n",
    "    if len(df) > 1020:\n",
    "        df = df.iloc[-1020:]  # Limit the data to the last 1020 entries if longer\n",
    "    _start = pd.Timestamp(\"2020-01-01\")  # Start date for the transformed data\n",
    "    _end = _start + pd.DateOffset(days=int(df.count()[0]) - 1)  # End date for the transformed data\n",
    "    date_idx = pd.date_range(start=_start, end=_end, freq=\"D\", name=\"ds\")  # Generate the date range\n",
    "    res_df = pd.DataFrame(data=[], index=date_idx).reset_index()  # Create an empty DataFrame with the date range\n",
    "    res_df[\"unique_id\"] = unique_id  # Add the unique ID column\n",
    "    res_df[\"y\"] = df.y.values  # Add the target variable column\n",
    "    return res_df\n",
    "\n",
    " \n",
    "(\n",
    "    spark.createDataFrame(create_m4_daily())  # Create a Spark DataFrame from the transformed data\n",
    "    .write.format(\"delta\").mode(\"overwrite\")  # Write the DataFrame to Delta format, overwriting any existing data\n",
    "    .saveAsTable(f\"{catalog}.{db}.m4_daily_train\")  # Save the table in the specified catalog and schema\n",
    ")\n",
    "\n",
    "# Print a confirmation message\n",
    "print(f\"Saved data to {catalog}.{db}.m4_daily_train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a7e525a-b5ff-4ea9-918a-acb35654886c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Monthly M4 data\n",
    "In our example notebooks, we primarily use daily time series. However, if you want to experiment with monthly time series, use the `m4_monthly_train` table generated by the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bef697e-1218-4c91-8b72-a236f9e1452c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_m4_monthly():\n",
    "    # Load the M4 monthly dataset\n",
    "    y_df, _, _ = M4.load(directory=str(pathlib.Path.home()), group=\"Monthly\")\n",
    "    # Create a list of unique IDs for the time series we want to sample\n",
    "    _ids = [f\"M{i}\" for i in range(1, n + 1)]\n",
    "    # Filter and transform the dataset based on the unique IDs\n",
    "    y_df = (\n",
    "        y_df.groupby(\"unique_id\")\n",
    "        .filter(lambda x: x.unique_id.iloc[0] in _ids)\n",
    "        .groupby(\"unique_id\")\n",
    "        .apply(transform_group_monthly)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return y_df\n",
    "\n",
    "\n",
    "def transform_group_monthly(df):\n",
    "    unique_id = df.unique_id.iloc[0]  # Get the unique ID of the current group\n",
    "    _cnt = 60  # Set the count for the number of months\n",
    "    _start = pd.Timestamp(\"2018-01-01\")  # Start date for the transformed data\n",
    "    _end = _start + pd.DateOffset(months=_cnt)  # End date for the transformed data\n",
    "    date_idx = pd.date_range(start=_start, end=_end, freq=\"M\", name=\"date\")  # Generate the date range for monthly data\n",
    "    _df = (\n",
    "        pd.DataFrame(data=[], index=date_idx)  # Create an empty DataFrame with the date range\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"date\"})  # Rename the index column to \"date\"\n",
    "    )\n",
    "    _df[\"unique_id\"] = unique_id  # Add the unique ID column\n",
    "    _df[\"y\"] = df[:60].y.values  # Add the target variable column, limited to 60 entries\n",
    "    return _df\n",
    "\n",
    "\n",
    "(\n",
    "    spark.createDataFrame(create_m4_monthly())  # Create a Spark DataFrame from the transformed data\n",
    "    .write.format(\"delta\").mode(\"overwrite\")  # Write the DataFrame to Delta format, overwriting any existing data\n",
    "    .saveAsTable(f\"{catalog}.{db}.m4_monthly_train\")  # Save the table in the specified catalog and schema\n",
    ")\n",
    "\n",
    "# Print a confirmation message\n",
    "print(f\"Saved data to {catalog}.{db}.m4_monthly_train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8711a0b5-f7c0-4c08-b1e1-fcc2f962c763",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Â© 2024 Databricks, Inc. All rights reserved. \n",
    "\n",
    "The sources in all notebooks in this directory and the sub-directories are provided subject to the Databricks License. All included or referenced third party libraries are subject to the licenses set forth below.\n",
    "\n",
    "| library                                | description             | license    | source                                              |\n",
    "|----------------------------------------|-------------------------|------------|-----------------------------------------------------|\n",
    "| datasetsforecast | Datasets for Time series forecasting | MIT | https://pypi.org/project/datasetsforecast/"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "99_data_preparation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
