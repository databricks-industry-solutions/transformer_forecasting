{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e261111f-e8b2-44c9-8761-9e57d82df0bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This is an example notebook that shows how to use [Moirai](https://github.com/SalesforceAIResearch/uni2ts) models on Databricks. The notebook loads the model, distributes the inference, registers the model, deploys the model and makes online forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7535fc20-46d5-4113-98a9-cde238b3991a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cluster setup\n",
    "\n",
    "We recommend using a cluster with [Databricks Runtime 16.4 LTS for ML](https://docs.databricks.com/en/release-notes/runtime/16.4lts-ml.html). The cluster can be single-node or multi-node with one or more GPU instances on each worker: e.g. [g5.12xlarge [A10G]](https://aws.amazon.com/ec2/instance-types/g5/) on AWS or [Standard_NV72ads_A10_v5](https://learn.microsoft.com/en-us/azure/virtual-machines/nva10v5-series) on Azure. This notebook will leverage [Pandas UDF](https://docs.databricks.com/en/udf/pandas.html) for distributing the inference tasks and utilizing all the available resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41f668e3-4a11-41c9-b9bd-27d3158b29f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Install package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf469d5c-3d10-472d-9757-ffb5d4caee00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install uni2ts==1.2.0 --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3c6cba5-0358-4f45-af35-8ce15d978267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare data \n",
    "We use [`datasetsforecast`](https://github.com/Nixtla/datasetsforecast/tree/main/) package to download M4 data. M4 dataset contains a set of time series which we use for testing. See the `data_preparation` notebook for a number of custom functions we wrote to convert M4 time series to an expected format.\n",
    "\n",
    "Make sure that the catalog and the schema already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4a4ded1-115a-4ce0-8323-ed6f6bce6937",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"tsfm\"  # Name of the catalog we use to manage our assets\n",
    "db = \"m4\"  # Name of the schema we use to manage our assets (e.g. datasets)\n",
    "n = 100  # Number of time series to sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "449e5752-d382-4d8f-9d02-2c45f750ffe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This cell runs the notebook ../data_preparation and creates the following tables with M4 data: \n",
    "# 1. {catalog}.{db}.m4_daily_train, \n",
    "# 2. {catalog}.{db}.m4_monthly_train\n",
    "dbutils.notebook.run(\"./99_data_preparation\", timeout_seconds=0, arguments={\"catalog\": catalog, \"db\": db, \"n\": n})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6b970b4-be4b-4083-8960-32f1f1f6a321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "# Make sure that the data exists\n",
    "df = spark.table(f'{catalog}.{db}.m4_daily_train')\n",
    "df = df.groupBy('unique_id').agg(collect_list('ds').alias('ds'), collect_list('y').alias('y'))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a98e05d-6482-4642-b84d-af3230967b3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Distribute Inference\n",
    "We use [Pandas UDF](https://docs.databricks.com/en/udf/pandas.html#iterator-of-series-to-iterator-of-series-udf) to distribute the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0906944f-481e-40b7-8a5f-537d2d600c67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from typing import Iterator\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# Function to create a Pandas UDF to generate horizon timestamps\n",
    "def create_get_horizon_timestamps(freq, prediction_length):\n",
    "    \"\"\"\n",
    "    Creates a Pandas UDF to generate horizon timestamps based on the given frequency and prediction length.\n",
    "\n",
    "    Parameters:\n",
    "    - freq (str): The frequency of the time series ('M' for monthly, 'D' for daily, etc.).\n",
    "    - prediction_length (int): The number of future timestamps to generate.\n",
    "\n",
    "    Returns:\n",
    "    - get_horizon_timestamps (function): A Pandas UDF function that generates horizon timestamps.\n",
    "    \"\"\"\n",
    "    \n",
    "    @pandas_udf('array<timestamp>')\n",
    "    def get_horizon_timestamps(batch_iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "        # Determine the offset for timestamp increments based on the frequency\n",
    "        one_ts_offset = pd.offsets.MonthEnd(1) if freq == \"M\" else pd.DateOffset(days=1)\n",
    "        \n",
    "        barch_horizon_timestamps = []\n",
    "        # Iterate over batches of series in the batch iterator\n",
    "        for batch in batch_iterator:\n",
    "            for series in batch:\n",
    "                timestamp = last = series.max()\n",
    "                horizon_timestamps = []\n",
    "                # Generate future timestamps based on the prediction length\n",
    "                for i in range(prediction_length):\n",
    "                    timestamp = timestamp + one_ts_offset\n",
    "                    horizon_timestamps.append(timestamp.to_numpy())\n",
    "                barch_horizon_timestamps.append(np.array(horizon_timestamps))\n",
    "        # Yield the generated horizon timestamps as a Pandas Series\n",
    "        yield pd.Series(barch_horizon_timestamps)\n",
    "\n",
    "    return get_horizon_timestamps\n",
    "\n",
    "# Function to create a Pandas UDF to generate forecasts\n",
    "def create_forecast_udf(repository, prediction_length, patch_size, num_samples):\n",
    "    \"\"\"\n",
    "    Creates a Pandas UDF to generate forecasts using a pre-trained model.\n",
    "\n",
    "    Parameters:\n",
    "    - repository (str): The path to the pre-trained model repository.\n",
    "    - prediction_length (int): The length of the forecast horizon.\n",
    "    - patch_size (int): The size of the patches for the model input.\n",
    "    - num_samples (int): The number of samples to generate for each forecast.\n",
    "\n",
    "    Returns:\n",
    "    - forecast_udf (function): A Pandas UDF function that generates forecasts.\n",
    "    \"\"\"\n",
    "    \n",
    "    @pandas_udf('array<double>')\n",
    "    def forecast_udf(bulk_iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "        ## Initialization step\n",
    "        import torch\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        from uni2ts.model.moirai import MoiraiForecast, MoiraiModule\n",
    "        \n",
    "        # Load the pre-trained model module from the repository\n",
    "        module = MoiraiModule.from_pretrained(repository)\n",
    "\n",
    "        ## Inference\n",
    "        for bulk in bulk_iterator:\n",
    "            median = []\n",
    "            for series in bulk:\n",
    "                # Initialize the forecast model with the loaded module and given parameters\n",
    "                model = MoiraiForecast(\n",
    "                    module=module,\n",
    "                    prediction_length=prediction_length,\n",
    "                    context_length=len(series),\n",
    "                    patch_size=patch_size,\n",
    "                    num_samples=num_samples,\n",
    "                    target_dim=1,\n",
    "                    feat_dynamic_real_dim=0,\n",
    "                    past_feat_dynamic_real_dim=0,\n",
    "                )\n",
    "                # Prepare the past target tensor. Shape: (batch, time, variate)\n",
    "                past_target = rearrange(\n",
    "                    torch.as_tensor(series, dtype=torch.float32), \"t -> 1 t 1\"\n",
    "                )\n",
    "                # Create a tensor indicating observed values. Shape: (batch, time, variate)\n",
    "                past_observed_target = torch.ones_like(past_target, dtype=torch.bool)\n",
    "                # Create a tensor indicating padding values. Shape: (batch, time)\n",
    "                past_is_pad = torch.zeros_like(past_target, dtype=torch.bool).squeeze(-1)\n",
    "                \n",
    "                # Generate the forecast\n",
    "                forecast = model(\n",
    "                    past_target=past_target,\n",
    "                    past_observed_target=past_observed_target,\n",
    "                    past_is_pad=past_is_pad,\n",
    "                )\n",
    "                # Append the median forecast of the first sample to the list\n",
    "                median.append(np.median(forecast[0], axis=0))\n",
    "        # Yield the generated forecasts as a Pandas Series\n",
    "        yield pd.Series(median)\n",
    "        \n",
    "    return forecast_udf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f53bdf3-8725-46f6-a4d5-77502519531f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We specify the requirements for our forecasts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20d4bbfe-a0bf-4c2f-8387-d4df2a34c6d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = \"moirai-1.1-R-small\"  # Alternatibely moirai-1.1-R-base, moirai-1.1-R-large\n",
    "prediction_length = 10  # Time horizon for forecasting\n",
    "num_samples = 10  # Number of forecast to generate. We will take median as our final forecast.\n",
    "patch_size = 32  # Patch size: choose from {\"auto\", 8, 16, 32, 64, 128}\n",
    "freq = \"D\" # Frequency of the time series\n",
    "device_count = torch.cuda.device_count()  # Number of GPUs available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bac9a9f8-60e6-417a-ba34-f34d44413924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's generate the forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b28b5c43-13da-4bc5-be0a-3ecbb558b687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the Pandas UDF for generating horizon timestamps using the specified frequency and prediction length\n",
    "get_horizon_timestamps = create_get_horizon_timestamps(freq=freq, prediction_length=prediction_length)\n",
    "\n",
    "# Create the Pandas UDF for generating forecasts using the specified model repository and forecast parameters\n",
    "forecast_udf = create_forecast_udf(\n",
    "  repository=f\"Salesforce/{model}\",  # Path to the pre-trained model repository\n",
    "  prediction_length=prediction_length,  # Length of the forecast horizon\n",
    "  patch_size=patch_size,  # Size of the patches for the model input\n",
    "  num_samples=num_samples,  # Number of samples to generate for each forecast\n",
    ")\n",
    "\n",
    "# Repartition the DataFrame to match the number of devices (for parallel processing) and select the required columns\n",
    "forecasts = df.repartition(device_count).select(\n",
    "  df.unique_id,  # Select the unique identifier for each time series\n",
    "  get_horizon_timestamps(df.ds).alias(\"ds\"),  # Generate horizon timestamps and alias as 'ds'\n",
    "  forecast_udf(df.y).alias(\"forecast\"),  # Generate forecasts and alias as 'forecast'\n",
    ")\n",
    "\n",
    "# Display the resulting DataFrame with unique_id, horizon timestamps, and forecasts\n",
    "display(forecasts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01b59396-77e9-4929-9d80-9a80b344790b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Register Model\n",
    "We will package our model using [`mlflow.pyfunc.PythonModel`](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html) and register this in Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15bd22b3-c36c-4fb1-a589-de7c5aa41505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "import numpy as np\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import DataType, Schema, TensorSpec\n",
    "\n",
    "# Set the MLflow registry URI to Databricks Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "experiment_name = \"/Shared/moirai/\"\n",
    "\n",
    "class MoiraiModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, repository):\n",
    "        \"\"\"\n",
    "        Initialize the MoiraiModel class by loading the pre-trained model from the given repository.\n",
    "        \n",
    "        Parameters:\n",
    "        - repository (str): The path to the pre-trained model repository.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        from uni2ts.model.moirai import MoiraiForecast, MoiraiModule\n",
    "        \n",
    "        # Load the pre-trained model module from the repository\n",
    "        self.module = MoiraiModule.from_pretrained(repository)\n",
    "  \n",
    "    def predict(self, context, input_data, params=None):\n",
    "        \"\"\"\n",
    "        Generate forecasts using the loaded model.\n",
    "        \n",
    "        Parameters:\n",
    "        - context: The context in which the model is being run.\n",
    "        - input_data: The input data for prediction, expected to be a time series.\n",
    "        - params: Additional parameters for prediction (not used here).\n",
    "        \n",
    "        Returns:\n",
    "        - forecast: The median forecast result as a NumPy array.\n",
    "        \"\"\"\n",
    "        from uni2ts.model.moirai import MoiraiForecast, MoiraiModule\n",
    "        \n",
    "        # Initialize the forecast model with the loaded module and given parameters\n",
    "        model = MoiraiForecast(\n",
    "            module=self.module,\n",
    "            prediction_length=10,  # Length of the forecast horizon\n",
    "            context_length=len(input_data),  # Context length is the length of the input data\n",
    "            patch_size=32,  # Size of the patches for the model input\n",
    "            num_samples=10,  # Number of samples to generate for each forecast\n",
    "            target_dim=1,  # Dimension of the target variable\n",
    "            feat_dynamic_real_dim=0,  # No dynamic real features\n",
    "            past_feat_dynamic_real_dim=0,  # No past dynamic real features\n",
    "        )\n",
    "        \n",
    "        # Prepare the past target tensor. Shape: (batch, time, variate)\n",
    "        past_target = rearrange(\n",
    "            torch.as_tensor(input_data, dtype=torch.float32), \"t -> 1 t 1\"\n",
    "        )\n",
    "        # Create a tensor indicating observed values. Shape: (batch, time, variate)\n",
    "        past_observed_target = torch.ones_like(past_target, dtype=torch.bool)\n",
    "        # Create a tensor indicating padding values. Shape: (batch, time)\n",
    "        past_is_pad = torch.zeros_like(past_target, dtype=torch.bool).squeeze(-1)\n",
    "        \n",
    "        # Generate the forecast\n",
    "        forecast = model(\n",
    "            past_target=past_target,\n",
    "            past_observed_target=past_observed_target,\n",
    "            past_is_pad=past_is_pad,\n",
    "        )\n",
    "        \n",
    "        # Return the median forecast of the first sample\n",
    "        return np.median(forecast[0], axis=0)\n",
    "\n",
    "# Initialize the MoiraiModel with the specified model repository\n",
    "pipeline = MoiraiModel(f\"Salesforce/{model}\")\n",
    "\n",
    "# Define the input and output schema for the model\n",
    "input_schema = Schema([TensorSpec(np.dtype(np.double), (-1,))])\n",
    "output_schema = Schema([TensorSpec(np.dtype(np.uint8), (-1,))])\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "# Example input data for model registration\n",
    "input_example = np.random.rand(52)\n",
    "\n",
    "# Define the registered model name\n",
    "registered_model_name = f\"{catalog}.{db}.moirai-1-1-r-small\"\n",
    "\n",
    "# set current experiment\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Log and register the model with MLflow\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.pyfunc.log_model(\n",
    "        \"model\",\n",
    "        python_model=pipeline,  # The custom Python model\n",
    "        registered_model_name=registered_model_name,  # The name under which to register the model\n",
    "        signature=signature,  # The model signature\n",
    "        input_example=input_example,  # An example of the input data\n",
    "        pip_requirements=[\n",
    "            \"uni2ts\",\n",
    "        ],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "135dfd05-634f-4088-89dc-76b036f664a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Reload Model\n",
    "Once the registration is complete, we will reload the model and generate forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "614e2f53-b98d-4daa-8a88-5251cecba152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "# Create an instance of the MlflowClient to interact with the MLflow tracking server\n",
    "mlflow_client = MlflowClient()\n",
    "\n",
    "def get_latest_model_version(mlflow_client, registered_model_name):\n",
    "    \"\"\"\n",
    "    Retrieve the latest version number of a registered model.\n",
    "    \n",
    "    Parameters:\n",
    "    - mlflow_client (MlflowClient): The MLflow client instance.\n",
    "    - registered_model_name (str): The name of the registered model.\n",
    "    \n",
    "    Returns:\n",
    "    - latest_version (int): The latest version number of the registered model.\n",
    "    \"\"\"\n",
    "    # Initialize the latest version to 1 (assuming at least one version exists)\n",
    "    latest_version = 1\n",
    "    \n",
    "    # Iterate over all model versions for the given registered model\n",
    "    for mv in mlflow_client.search_model_versions(f\"name='{registered_model_name}'\"):\n",
    "        # Convert the version to an integer\n",
    "        version_int = int(mv.version)\n",
    "        \n",
    "        # Update the latest version if a higher version is found\n",
    "        if version_int > latest_version:\n",
    "            latest_version = version_int\n",
    "            \n",
    "    # Return the latest version number\n",
    "    return latest_version\n",
    "\n",
    "# Get the latest version of the registered model\n",
    "model_version = get_latest_model_version(mlflow_client, registered_model_name)\n",
    "\n",
    "# Construct the URI for the logged model using the registered model name and latest version\n",
    "logged_model = f\"models:/{registered_model_name}/{model_version}\"\n",
    "\n",
    "# Load the model as a PyFuncModel from the logged model URI\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "\n",
    "# Create random input data (52 data points)\n",
    "input_data = np.random.rand(52)\n",
    "\n",
    "# Generate forecasts using the loaded model\n",
    "loaded_model.predict(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcaab92f-ebcb-45cd-92de-e2f27153255b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy Model\n",
    "We will deploy our model behind a real-time endpoint of [Databricks Mosaic AI Model Serving](https://www.databricks.com/product/model-serving)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92f943ce-aa9c-4759-835f-69f26003f6b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# With the token, you can create our authorization header for our subsequent REST calls\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\n",
    "headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Next you need an endpoint at which to execute your request which you can get from the notebook's tags collection\n",
    "java_tags = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags()\n",
    "\n",
    "# This object comes from the Java CM - Convert the Java Map object to a Python dictionary\n",
    "tags = sc._jvm.scala.collection.JavaConverters.mapAsJavaMapConverter(java_tags).asJava()\n",
    "\n",
    "# Lastly, extract the Databricks instance (domain name) from the dictionary\n",
    "instance = tags[\"browserHostName\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32da2d6f-e292-4d35-8aa0-8999027c357b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "model_serving_endpoint_name = \"moirai-1-1-r-small\"\n",
    "\n",
    "# auto_capture_config specifies where the inference logs should be written\n",
    "my_json = {\n",
    "    \"name\": model_serving_endpoint_name,\n",
    "    \"config\": {\n",
    "        \"served_models\": [\n",
    "            {\n",
    "                \"model_name\": registered_model_name,\n",
    "                \"model_version\": model_version,\n",
    "                \"workload_type\": \"GPU_SMALL\",\n",
    "                \"workload_size\": \"Small\",\n",
    "                \"scale_to_zero_enabled\": \"true\",\n",
    "            }\n",
    "        ],\n",
    "        \"auto_capture_config\": {\n",
    "            \"catalog_name\": catalog,\n",
    "            \"schema_name\": db,\n",
    "            \"table_name_prefix\": model_serving_endpoint_name,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Make sure to drop the inference table of it exists\n",
    "_ = spark.sql(\n",
    "    f\"DROP TABLE IF EXISTS {catalog}.{db}.`{model_serving_endpoint_name}_payload`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "176644f0-a7b0-464f-bfe4-be96f20c2476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to create an endpoint in Model Serving and deploy the model behind it\n",
    "def func_create_endpoint(model_serving_endpoint_name):\n",
    "    # get endpoint status\n",
    "    endpoint_url = f\"https://{instance}/api/2.0/serving-endpoints\"\n",
    "    url = f\"{endpoint_url}/{model_serving_endpoint_name}\"\n",
    "    r = requests.get(url, headers=headers)\n",
    "    if \"RESOURCE_DOES_NOT_EXIST\" in r.text:\n",
    "        print(\n",
    "            \"Creating this new endpoint: \",\n",
    "            f\"https://{instance}/serving-endpoints/{model_serving_endpoint_name}/invocations\",\n",
    "        )\n",
    "        re = requests.post(endpoint_url, headers=headers, json=my_json)\n",
    "    else:\n",
    "        new_model_version = (my_json[\"config\"])[\"served_models\"][0][\"model_version\"]\n",
    "        print(\n",
    "            \"This endpoint existed previously! We are updating it to a new config with new model version: \",\n",
    "            new_model_version,\n",
    "        )\n",
    "        # update config\n",
    "        url = f\"{endpoint_url}/{model_serving_endpoint_name}/config\"\n",
    "        re = requests.put(url, headers=headers, json=my_json[\"config\"])\n",
    "        # wait till new config file in place\n",
    "        import time, json\n",
    "\n",
    "        # get endpoint status\n",
    "        url = f\"https://{instance}/api/2.0/serving-endpoints/{model_serving_endpoint_name}\"\n",
    "        retry = True\n",
    "        total_wait = 0\n",
    "        while retry:\n",
    "            r = requests.get(url, headers=headers)\n",
    "            assert (\n",
    "                r.status_code == 200\n",
    "            ), f\"Expected an HTTP 200 response when accessing endpoint info, received {r.status_code}\"\n",
    "            endpoint = json.loads(r.text)\n",
    "            if \"pending_config\" in endpoint.keys():\n",
    "                seconds = 10\n",
    "                print(\"New config still pending\")\n",
    "                if total_wait < 6000:\n",
    "                    # if less the 10 mins waiting, keep waiting\n",
    "                    print(f\"Wait for {seconds} seconds\")\n",
    "                    print(f\"Total waiting time so far: {total_wait} seconds\")\n",
    "                    time.sleep(10)\n",
    "                    total_wait += seconds\n",
    "                else:\n",
    "                    print(f\"Stopping,  waited for {total_wait} seconds\")\n",
    "                    retry = False\n",
    "            else:\n",
    "                print(\"New config in place now!\")\n",
    "                retry = False\n",
    "\n",
    "    assert (\n",
    "        re.status_code == 200\n",
    "    ), f\"Expected an HTTP 200 response, received {re.status_code}\"\n",
    "\n",
    "# Function to delete the endpoint from Model Serving\n",
    "def func_delete_model_serving_endpoint(model_serving_endpoint_name):\n",
    "    endpoint_url = f\"https://{instance}/api/2.0/serving-endpoints\"\n",
    "    url = f\"{endpoint_url}/{model_serving_endpoint_name}\"\n",
    "    response = requests.delete(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            f\"Request failed with status {response.status_code}, {response.text}\"\n",
    "        )\n",
    "    else:\n",
    "        print(model_serving_endpoint_name, \"endpoint is deleted!\")\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0033f686-f0a6-4e87-9cbc-90961e6e49a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create an endpoint. This may take some time.\n",
    "func_create_endpoint(model_serving_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31fc289f-02ef-4445-810e-57291f810312",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import mlflow\n",
    "import requests\n",
    "\n",
    "def wait_for_endpoint():\n",
    "    \"\"\"\n",
    "    Waits for a model serving endpoint to become ready.\n",
    "\n",
    "    This function continuously polls the serving endpoint's status and waits until the endpoint is ready.\n",
    "    \"\"\"\n",
    "    # Construct the base URL for the serving endpoint API\n",
    "    endpoint_url = f\"https://{instance}/api/2.0/serving-endpoints\"\n",
    "    \n",
    "    while True:\n",
    "        # Construct the full URL for the specific model serving endpoint\n",
    "        url = f\"{endpoint_url}/{model_serving_endpoint_name}\"\n",
    "        \n",
    "        # Send a GET request to the endpoint URL with the required headers\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        # Assert that the response status code is 200 (OK)\n",
    "        assert (\n",
    "            response.status_code == 200\n",
    "        ), f\"Expected an HTTP 200 response, received {response.status_code}\\n{response.text}\"\n",
    "        \n",
    "        # Extract the 'ready' status from the JSON response\n",
    "        status = response.json().get(\"state\", {}).get(\"ready\", {})\n",
    "        \n",
    "        # Check if the status is \"READY\"\n",
    "        if status == \"READY\":\n",
    "            # Print the status and a separator line, then exit the function\n",
    "            print(status)\n",
    "            print(\"-\" * 80)\n",
    "            return\n",
    "        else:\n",
    "            # Print a message indicating the endpoint is not ready and wait for 5 minutes (300 seconds)\n",
    "            print(f\"Endpoint not ready ({status}), waiting 5 minutes\")\n",
    "            time.sleep(300)\n",
    "\n",
    "# Get the API URL for the current Databricks instance\n",
    "api_url = mlflow.utils.databricks_utils.get_webapp_url()\n",
    "\n",
    "# Call the function to wait for the endpoint to become ready\n",
    "wait_for_endpoint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b4882f-1305-4528-9a33-4c140c424016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Online Forecast\n",
    "Once the endpoint is ready, let's send a request to the model and generate an online forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b55a0a9-9c99-416f-b63f-ef5ac577f260",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Construct the endpoint URL for model invocation using the provided instance and model serving endpoint name.\n",
    "# This URL is used to send data to the model and get predictions.\n",
    "endpoint_url = f\"https://{instance}/serving-endpoints/{model_serving_endpoint_name}/invocations\"\n",
    "\n",
    "# Retrieve the Databricks API token using dbutils (a utility available in Databricks notebooks).\n",
    "# This token is used for authentication when making requests to the endpoint.\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "def forecast(input_data, url=endpoint_url, databricks_token=token):\n",
    "    \"\"\"\n",
    "    Send input data to the model serving endpoint and retrieve the forecast.\n",
    "\n",
    "    Parameters:\n",
    "    - input_data (numpy.ndarray): The input data to be sent to the model.\n",
    "    - url (str): The endpoint URL for model invocation.\n",
    "    - databricks_token (str): The Databricks API token for authentication.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The JSON response from the model containing the forecast.\n",
    "    \"\"\"\n",
    "    # Set the request headers, including the authorization token and content type.\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {databricks_token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    \n",
    "    # Convert the input data to a list and create the request body.\n",
    "    body = {\"inputs\": input_data.tolist()}\n",
    "    \n",
    "    # Serialize the request body to a JSON formatted string.\n",
    "    data = json.dumps(body)\n",
    "    \n",
    "    # Send a POST request to the endpoint URL with the headers and serialized data.\n",
    "    response = requests.request(method=\"POST\", headers=headers, url=url, data=data)\n",
    "    \n",
    "    # Check if the response status code is not 200 (OK), raise an exception if the request failed.\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            f\"Request failed with status {response.status_code}, {response.text}\"\n",
    "        )\n",
    "    \n",
    "    # Return the JSON response from the model containing the forecast.\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f514b7cc-1bd7-4738-96bf-bf67579cfe59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Send request to the endpoint\n",
    "input_data = np.random.rand(52)\n",
    "forecast(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68982227-042c-4b93-a548-702edc92e395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delete the serving endpoint\n",
    "func_delete_model_serving_endpoint(model_serving_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58e7d900-3415-4df5-8252-9998d0e04651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "© 2024 Databricks, Inc. All rights reserved. \n",
    "\n",
    "The sources in all notebooks in this directory and the sub-directories are provided subject to the Databricks License. All included or referenced third party libraries are subject to the licenses set forth below."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02a_moirai_load_inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
