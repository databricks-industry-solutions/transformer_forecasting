{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f907f5b-3971-4110-b096-bc3df955a16b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This is an example notebook that shows how to use [TimesFM](https://github.com/google-research/timesfm) models on Databricks. The notebook loads the model, distributes the inference, registers the model, deploys the model and makes online forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "557ba6c8-9601-4769-8c35-461ac96f2275",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cluster setup\n",
    "\n",
    "We recommend using a cluster with [Databricks Runtime 15.4 LTS for ML](https://docs.databricks.com/en/release-notes/runtime/14.3lts-ml.html). The cluster can be single-node or multi-node with one or more GPU instances on each worker: e.g. [g5.2xlarge [A10G]](https://aws.amazon.com/ec2/instance-types/g5/) on AWS or [Standard_NV72ads_A10_v5](https://learn.microsoft.com/en-us/azure/virtual-machines/nva10v5-series) on Azure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b222934c-9fab-464d-b6b0-320917b35bc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Install package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0ed1ef1-12ad-4eee-a5e5-bd9b337e0ec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install torch==2.3.1+cu121 torchvision>=0.18.0 numpy==1.26.4 pandas==2.1.4 pyarrow==14.0.1 pyarrow-hotfix==0.6 timesfm[torch]==1.3.0 --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a2e0ce2-310c-4096-9248-aa779a45ab1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Set the logging level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb731407-720f-4e4c-9b66-83e85e19cd39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = spark._jvm.org.apache.log4j\n",
    "\n",
    "# Setting the logging level to ERROR for the \"py4j.java_gateway\" logger\n",
    "# This reduces the verbosity of the logs by only showing error messages\n",
    "logging.getLogger(\"py4j.java_gateway\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"py4j.clientserver\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fb3aaca-bc46-431c-b82d-1620374db556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare data \n",
    "We use [`datasetsforecast`](https://github.com/Nixtla/datasetsforecast/tree/main/) package to download M4 data. M4 dataset contains a set of time series which we use for testing. See the `data_preparation` notebook for a number of custom functions we wrote to convert M4 time series to an expected format.\n",
    "\n",
    "Make sure that the catalog and the schema already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03815a19-584c-48df-8849-3c8cb8813625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"tsfm\"  # Name of the catalog we use to manage our assets\n",
    "db = \"m4\"  # Name of the schema we use to manage our assets (e.g. datasets)\n",
    "n = 100  # Number of time series to sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "907058e8-18da-4746-a5cc-1324b7630f75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This cell runs the notebook ../data_preparation and creates the following tables with M4 data: \n",
    "# 1. {catalog}.{db}.m4_daily_train\n",
    "# 2. {catalog}.{db}.m4_monthly_train\n",
    "dbutils.notebook.run(\"./99_data_preparation\", timeout_seconds=0, arguments={\"catalog\": catalog, \"db\": db, \"n\": n})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf8644e5-d412-46a3-8bd8-6880e12660c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make sure that the data exists\n",
    "df = spark.table(f'{catalog}.{db}.m4_daily_train').toPandas()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bcada9f-8782-4a29-840e-0ef66995aa88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Distribute Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d8dd9eb-04f4-4eb3-aa1b-fce3a6fb8cd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Distribution of the inference is managed by TimesFM so we don't need to use Pandas UDF. See the Github [repository](https://github.com/google-research/timesfm/tree/master?tab=readme-ov-file#initialize-the-model-and-load-a-checkpoint) of TimesFM for detailed description of the input parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e23e2e7b-2be4-4fb1-b0a5-91a63202b5ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import timesfm\n",
    "\n",
    "# Initialize the TimesFm model with specified parameters.\n",
    "tfm = timesfm.TimesFm(\n",
    "    hparams=timesfm.TimesFmHparams(\n",
    "        backend=\"gpu\",\n",
    "        per_core_batch_size=32,\n",
    "        horizon_len=10,\n",
    "    ),\n",
    "    checkpoint=timesfm.TimesFmCheckpoint(\n",
    "        huggingface_repo_id=\"google/timesfm-1.0-200m-pytorch\"),\n",
    ")\n",
    "\n",
    "# Generate forecasts on the input DataFrame.\n",
    "forecast_df = tfm.forecast_on_df(\n",
    "    inputs=df,  # The input DataFrame containing the time series data.\n",
    "    freq=\"D\",  # Frequency of the time series data, set to daily.\n",
    "    value_name=\"y\",  # Column name in the DataFrame containing the values to forecast.\n",
    "    num_jobs=-1,  # Number of parallel jobs to run, set to -1 to use all available processors.\n",
    ")\n",
    "\n",
    "# Display the forecast DataFrame.\n",
    "display(forecast_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1439ea90-7248-4782-83a8-eae21c58fe78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Register Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f9d4b8a-d3d5-451f-b74e-50f637081565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We should ensure that any non-serializable attributes (like the timesfm model in TimesFMModel class) are not included in the serialization process. One common approach is to override the __getstate__ and __setstate__ methods in the class to manage what gets pickled. This modification ensures that the timesfm model is not included in the serialization process, thus avoiding the error. The load_model method is called to load the model when needed, such as during prediction or after deserialization.\n",
    "\n",
    "We will package our model using [`mlflow.pyfunc.PythonModel`](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html) and register this in Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6eb66fa-43ba-4fcb-a742-c03631d1d850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "import numpy as np\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import DataType, Schema, TensorSpec\n",
    "\n",
    "# Set the MLflow registry URI to Databricks Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "experiment_name = \"/Shared/timesfm/\"\n",
    "\n",
    "# Define a custom MLflow Python model class for TimesFM\n",
    "class TimesFMModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, repository):\n",
    "        self.repository = repository  # Store the repository ID for the model checkpoint\n",
    "        self.tfm = None  # Initialize the model attribute to None\n",
    "\n",
    "    def load_model(self):\n",
    "        # Initialize the TimesFm model with specified parameters\n",
    "        import timesfm\n",
    "        self.tfm = timesfm.TimesFm(\n",
    "            hparams=timesfm.TimesFmHparams(\n",
    "                backend=\"gpu\",\n",
    "                per_core_batch_size=32,\n",
    "                horizon_len=10,\n",
    "                ),\n",
    "            checkpoint=timesfm.TimesFmCheckpoint(\n",
    "                huggingface_repo_id=self.repository\n",
    "                ),\n",
    "            )\n",
    "        \n",
    "    def predict(self, context, input_df, params=None):\n",
    "        # Load the model if it hasn't been loaded yet\n",
    "        if self.tfm is None:\n",
    "            self.load_model()\n",
    "        # Generate forecasts on the input DataFrame\n",
    "        forecast_df = self.tfm.forecast_on_df(\n",
    "            inputs=input_df,  # Input DataFrame containing the time series data.\n",
    "            freq=\"D\",  # Frequency of the time series data, set to daily.\n",
    "            value_name=\"y\",  # Column name in the DataFrame containing the values to forecast.\n",
    "            num_jobs=-1,  # Number of parallel jobs to run, set to -1 to use all available processors.\n",
    "        )\n",
    "        return forecast_df  # Return the forecast DataFrame\n",
    "\n",
    "\n",
    "# Initialize the custom TimesFM model with the specified repository ID\n",
    "pipeline = TimesFMModel(\"google/timesfm-1.0-200m-pytorch\")\n",
    "# Infer the model signature based on input and output DataFrames\n",
    "signature = infer_signature(\n",
    "    model_input=df,  # Input DataFrame for the model\n",
    "    model_output=pipeline.predict(None, df),  # Output DataFrame from the model\n",
    ")\n",
    "\n",
    "# Define the registered model name using variables for catalog and database\n",
    "registered_model_name = f\"{catalog}.{db}.timesfm-1-200m-pytorch\"\n",
    "\n",
    "# set current experiment\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Start an MLflow run to log and register the model\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.pyfunc.log_model(\n",
    "        \"model\",  # The artifact path where the model is logged\n",
    "        python_model=pipeline,  # The custom Python model to log\n",
    "        registered_model_name=registered_model_name,  # The name to register the model under\n",
    "        signature=signature,  # The model signature\n",
    "        input_example=df,  # An example input to log with the model\n",
    "        pip_requirements=[\"timesfm[torch]\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dab3176c-d7c6-49db-be24-d42248104e9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Reload Model\n",
    "Once the registration is complete, we will reload the model and generate forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f0718b1-488d-472f-86fc-a3dc648cb405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "# Define a function to get the latest version number of a registered model\n",
    "def get_latest_model_version(client, registered_model_name):\n",
    "    latest_version = 1  # Initialize the latest version number to 1\n",
    "    # Iterate through all model versions of the specified registered model\n",
    "    for mv in client.search_model_versions(f\"name='{registered_model_name}'\"):\n",
    "        version_int = int(mv.version)  # Convert the version number to an integer\n",
    "        if version_int > latest_version:  # Check if the current version is greater than the latest version\n",
    "            latest_version = version_int  # Update the latest version number\n",
    "    return latest_version  # Return the latest version number\n",
    "\n",
    "# Get the latest version number of the specified registered model\n",
    "model_version = get_latest_model_version(client, registered_model_name)\n",
    "# Construct the model URI using the registered model name and the latest version number\n",
    "logged_model = f\"models:/{registered_model_name}/{model_version}\"\n",
    "\n",
    "# Load the model as a PyFuncModel\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "# Generate forecasts using the loaded model on the input DataFrame\n",
    "loaded_model.predict(df)  # Use the loaded model to make predictions on the input DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92efcdcd-38a3-4a08-ba39-e4f0c970f492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy Model\n",
    "We will deploy our model behind a real-time endpoint of [Databricks Mosaic AI Model Serving](https://www.databricks.com/product/model-serving)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad5a788-197e-40c0-a3da-01536bd472a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# With the token, you can create our authorization header for our subsequent REST calls\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\n",
    "headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Next you need an endpoint at which to execute your request which you can get from the notebook's tags collection\n",
    "java_tags = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags()\n",
    "\n",
    "# This object comes from the Java CM - Convert the Java Map opject to a Python dictionary\n",
    "tags = sc._jvm.scala.collection.JavaConversions.mapAsJavaMap(java_tags)\n",
    "\n",
    "# Lastly, extract the Databricks instance (domain name) from the dictionary\n",
    "instance = tags[\"browserHostName\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43c398d9-014c-4068-b435-4a9a7c9cdc8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "model_serving_endpoint_name = \"timesfm-1-200m\"\n",
    "\n",
    "my_json = {\n",
    "    \"name\": model_serving_endpoint_name,\n",
    "    \"config\": {\n",
    "        \"served_models\": [\n",
    "            {\n",
    "                \"model_name\": registered_model_name,\n",
    "                \"model_version\": model_version,\n",
    "                \"workload_type\": \"GPU_SMALL\",\n",
    "                \"workload_size\": \"Small\",\n",
    "                \"scale_to_zero_enabled\": \"true\",\n",
    "                \"environment_vars\":{\n",
    "                    \"JAX_PLATFORMS\": \"gpu\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"auto_capture_config\": {\n",
    "            \"catalog_name\": catalog,\n",
    "            \"schema_name\": db,\n",
    "            \"table_name_prefix\": model_serving_endpoint_name,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Make sure to drop the inference table of it exists\n",
    "_ = spark.sql(\n",
    "    f\"DROP TABLE IF EXISTS {catalog}.{db}.`{model_serving_endpoint_name}_payload`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2dbb48b-2f22-401f-bff1-53c1b424f7eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to create an endpoint in Model Serving and deploy the model behind it\n",
    "def func_create_endpoint(model_serving_endpoint_name):\n",
    "    # get endpoint status\n",
    "    endpoint_url = f\"https://{instance}/api/2.0/serving-endpoints\"\n",
    "    url = f\"{endpoint_url}/{model_serving_endpoint_name}\"\n",
    "    r = requests.get(url, headers=headers)\n",
    "    if \"RESOURCE_DOES_NOT_EXIST\" in r.text:\n",
    "        print(\n",
    "            \"Creating this new endpoint: \",\n",
    "            f\"https://{instance}/serving-endpoints/{model_serving_endpoint_name}/invocations\",\n",
    "        )\n",
    "        re = requests.post(endpoint_url, headers=headers, json=my_json)\n",
    "    else:\n",
    "        new_model_version = (my_json[\"config\"])[\"served_models\"][0][\"model_version\"]\n",
    "        print(\n",
    "            \"This endpoint existed previously! We are updating it to a new config with new model version: \",\n",
    "            new_model_version,\n",
    "        )\n",
    "        # update config\n",
    "        url = f\"{endpoint_url}/{model_serving_endpoint_name}/config\"\n",
    "        re = requests.put(url, headers=headers, json=my_json[\"config\"])\n",
    "        # wait till new config file in place\n",
    "        import time, json\n",
    "\n",
    "        # get endpoint status\n",
    "        url = f\"https://{instance}/api/2.0/serving-endpoints/{model_serving_endpoint_name}\"\n",
    "        retry = True\n",
    "        total_wait = 0\n",
    "        while retry:\n",
    "            r = requests.get(url, headers=headers)\n",
    "            assert (\n",
    "                r.status_code == 200\n",
    "            ), f\"Expected an HTTP 200 response when accessing endpoint info, received {r.status_code}\"\n",
    "            endpoint = json.loads(r.text)\n",
    "            if \"pending_config\" in endpoint.keys():\n",
    "                seconds = 10\n",
    "                print(\"New config still pending\")\n",
    "                if total_wait < 6000:\n",
    "                    # if less the 10 mins waiting, keep waiting\n",
    "                    print(f\"Wait for {seconds} seconds\")\n",
    "                    print(f\"Total waiting time so far: {total_wait} seconds\")\n",
    "                    time.sleep(10)\n",
    "                    total_wait += seconds\n",
    "                else:\n",
    "                    print(f\"Stopping,  waited for {total_wait} seconds\")\n",
    "                    retry = False\n",
    "            else:\n",
    "                print(\"New config in place now!\")\n",
    "                retry = False\n",
    "\n",
    "    assert (\n",
    "        re.status_code == 200\n",
    "    ), f\"Expected an HTTP 200 response, received {re.status_code}\"\n",
    "\n",
    "# Function to delete the endpoint from Model Serving\n",
    "def func_delete_model_serving_endpoint(model_serving_endpoint_name):\n",
    "    endpoint_url = f\"https://{instance}/api/2.0/serving-endpoints\"\n",
    "    url = f\"{endpoint_url}/{model_serving_endpoint_name}\"\n",
    "    response = requests.delete(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            f\"Request failed with status {response.status_code}, {response.text}\"\n",
    "        )\n",
    "    else:\n",
    "        print(model_serving_endpoint_name, \"endpoint is deleted!\")\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1234e50-9746-4fbd-8904-c45e7b08d082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create an endpoint. This may take some time.\n",
    "func_create_endpoint(model_serving_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "048cd34f-5269-45e0-a8fd-a5ef7d89d3ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time, mlflow\n",
    "\n",
    "# Define a function to wait for a serving endpoint to be ready\n",
    "def wait_for_endpoint():\n",
    "    endpoint_url = f\"https://{instance}/api/2.0/serving-endpoints\"  # Construct the base URL for the serving endpoints API\n",
    "    while True:  # Infinite loop to repeatedly check the status of the endpoint\n",
    "        url = f\"{endpoint_url}/{model_serving_endpoint_name}\"  # Construct the URL for the specific model serving endpoint\n",
    "        response = requests.get(url, headers=headers)  # Send a GET request to the endpoint URL with the necessary headers\n",
    "        \n",
    "        # Ensure the response status code is 200 (OK)\n",
    "        assert (\n",
    "            response.status_code == 200\n",
    "        ), f\"Expected an HTTP 200 response, received {response.status_code}\\n{response.text}\"\n",
    "\n",
    "        # Extract the status of the endpoint from the response JSON\n",
    "        status = response.json().get(\"state\", {}).get(\"ready\", {})\n",
    "        # print(\"status\",status)  # Optional: Print the status for debugging purposes\n",
    "        \n",
    "        # Check if the endpoint status is \"READY\"\n",
    "        if status == \"READY\":\n",
    "            print(status)  # Print the status if the endpoint is ready\n",
    "            print(\"-\" * 80)  # Print a separator line for clarity\n",
    "            return  # Exit the function when the endpoint is ready\n",
    "        else:\n",
    "            # Print a message indicating the endpoint is not ready and wait for 5 minutes\n",
    "            print(f\"Endpoint not ready ({status}), waiting 5 minutes\")\n",
    "            time.sleep(300)  # Wait for 300 seconds before checking again\n",
    "\n",
    "# Get the Databricks web application URL using MLflow utility function\n",
    "api_url = mlflow.utils.databricks_utils.get_webapp_url()\n",
    "\n",
    "# Call the wait_for_endpoint function to wait for the serving endpoint to be ready\n",
    "wait_for_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69fbd2df-64b6-4743-9f1c-a3a42a89c700",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Online Forecast\n",
    "Once the endpoint is ready, let's send a request to the model and generate an online forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a710218-535e-41e4-807b-8ea5c94f271e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace URL with the end point invocation url you get from Model Seriving page.\n",
    "endpoint_url = f\"https://{instance}/serving-endpoints/{model_serving_endpoint_name}/invocations\"\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "def forecast(input_data, url=endpoint_url, databricks_token=token):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {databricks_token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    body = {\"inputs\": input_data.to_dict(orient='records')}\n",
    "    data = json.dumps(body)\n",
    "    response = requests.request(method=\"POST\", headers=headers, url=url, data=data)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            f\"Request failed with status {response.status_code}, {response.text}\"\n",
    "        )\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26214ae3-1896-47e0-8a34-ea2bc75363b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Send request to the endpoint\n",
    "df = spark.table(f'{catalog}.{db}.m4_daily_train').toPandas()\n",
    "df['ds'] = df['ds'].astype(str)  # Convert Timestamp to string\n",
    "forecast(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef7a43e8-e693-48c1-94b6-791ecc20cb9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delete the serving endpoint\n",
    "func_delete_model_serving_endpoint(model_serving_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5e0d54a-043e-4bc7-b82a-d42d34817b3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "© 2024 Databricks, Inc. All rights reserved. \n",
    "\n",
    "The sources in all notebooks in this directory and the sub-directories are provided subject to the Databricks License. All included or referenced third party libraries are subject to the licenses set forth below."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "05__timesfm_load_inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
