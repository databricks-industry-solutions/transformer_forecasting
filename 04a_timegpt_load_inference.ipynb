{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c56d2343-a450-41a6-8e11-32238f0bf350",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This is an example notebook that shows how to use Foundational Model Time-Series [TimeGPT](https://docs.nixtla.io/) models on Databricks. \n",
    "The notebook loads the model, distributes the inference, registers the model, deploys the model and makes online forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b5533e4-498c-40eb-b0a0-095364187be6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pre-requisites to URL and API key for AzureAI\n",
    "Here are the prerequisites:\n",
    "1. Access the Azure portal for your Azure subscription.\n",
    "2. Create an Azure AI Studio hub.\n",
    "3. Once deployed, launch the Azure AI Studio and navigate to the model catalog.\n",
    "4. From the model catalog, select Nixtla's TimeGEN-1 forecasting model.\n",
    "5. Deploy the model to a project.  If you do not have a project, you can create a new one here.\n",
    "6. From the model deployment page, copy the Endpoint Target URI and Key for use below (once the endpoint has completed deployment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61ca182e-3c6d-4a5c-bfb7-d284a3f1ea3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cluster setup\n",
    "\n",
    "TimeGPT is accessible through an API as a service, so the actual compute for inference or fine-tuning will not take place on Databricks. For this reason a GPU cluster is not necessary and we recommend using a cluster with [Databricks Runtime 16.4 LTS for ML](https://docs.databricks.com/en/release-notes/runtime/16.4lts-ml.html) with CPUs. This notebook will leverage [Pandas UDF](https://docs.databricks.com/en/udf/pandas.html) for distributing the inference tasks and utilizing all the available resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf694bad-36a9-4613-ba63-e2cc1d7d18ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Install package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "394dca9f-bdd2-4913-b137-5fe7e5359919",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Libraries"
    }
   },
   "outputs": [],
   "source": [
    "%pip install nixtla --quiet\n",
    "%pip install --upgrade mlflow --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "880602e7-c120-4968-9957-d30e34b2f589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Set the logging level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "806ea871-f478-491d-9f45-d3df95af6834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "# Setting the logging level to ERROR for the \"py4j.java_gateway\" logger\n",
    "# This reduces the verbosity of the logs by only showing error messages\n",
    "logging.getLogger(\"py4j.java_gateway\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"py4j.clientserver\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab590b1-bcb8-42fc-8b29-ce1e43e150c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Record the TimeGen-1 Endpoint Target URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9823e5fb-f981-454b-8f67-160b9abd11b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_url = '<input timegen-1 endpoint uri here>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9994a427-2d8a-41ca-811a-9ad5cdb2b47c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Add the API key as a secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4af4092-f48d-4134-97e8-07599fdd173c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "key_name = \"api_key\"\n",
    "scope_name = \"time-gpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aa88eae-1027-4b60-af72-b47aa376593c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If this is your first time running the notebook and you still don't have your credential managed in the secret, uncomment and run the following cell. Read more about Databricks secrets management [here](https://docs.databricks.com/en/security/secrets/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87373b1c-a6b2-41ba-80e3-0e6c0efae14f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# put the key in secret \n",
    "try: \n",
    "  w.secrets.create_scope(scope=scope_name)\n",
    "except:\n",
    "  pass\n",
    "w.secrets.put_secret(scope=scope_name, key=key_name, string_value='<input timegen-1 api key here>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33273505-857c-4ae2-8c04-bf8a6aa108f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare data \n",
    "We use [`datasetsforecast`](https://github.com/Nixtla/datasetsforecast/tree/main/) package to download M4 data. M4 dataset contains a set of time series which we use for testing. See the `data_preparation` notebook for a number of custom functions we wrote to convert M4 time series to an expected format.\n",
    "\n",
    "Make sure that the catalog and the schema already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03e67da9-b59a-422c-89e7-3787fa2c9542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"tsfm\"  # Name of the catalog we use to manage our assets\n",
    "db = \"m4\"  # Name of the schema we use to manage our assets (e.g. datasets)\n",
    "n = 10  # Number of time series to sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8ff2985-93fb-4ce5-8eda-06b9c9815dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This cell runs the notebook ../data_preparation and creates the following tables with M4 data: \n",
    "# 1. {catalog}.{db}.m4_daily_train, \n",
    "# 2. {catalog}.{db}.m4_monthly_train\n",
    "dbutils.notebook.run(\"./99_data_preparation\", timeout_seconds=0, arguments={\"catalog\": catalog, \"db\": db, \"n\": n})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54e106ff-2e4a-4b6f-81e0-f8be7cb2df84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "# Make sure that the data exists\n",
    "df = spark.table(f'{catalog}.{db}.m4_daily_train')\n",
    "df = df.groupBy('unique_id').agg(collect_list('ds').alias('ds'), collect_list('y').alias('y'))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8839843-783d-4754-8e75-d2d92ef47142",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Distribute Inference\n",
    "We use [Pandas UDF](https://docs.databricks.com/en/udf/pandas.html#iterator-of-series-to-iterator-of-series-udf) to distribute the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bc2f4f7-643a-4892-b150-7a495d48f007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Iterator,Tuple\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "\n",
    "## Function to select a single time series from the prepared dataset\n",
    "def get_single_time_series(unique_id):\n",
    "  # Filter the DataFrame to get records with the specified unique_id and convert to a pandas DataFrame\n",
    "  pdf = df.filter(df.unique_id == unique_id).toPandas()\n",
    "  # Create a dictionary with timestamp and value columns\n",
    "  pdf = {\n",
    "    \"timestamp\" : list(pdf['ds'][0]),\n",
    "    \"value\" : list(pdf['y'][0])\n",
    "  }\n",
    "  # Return a new pandas DataFrame created from the dictionary\n",
    "  return pd.DataFrame(pdf)\n",
    "\n",
    "\n",
    "## Function to create a Pandas UDF to generate forecasts given a time series history\n",
    "def create_forecast_udf(model_url, api_key, prediction_length=12):\n",
    "\n",
    "  @pandas_udf('struct<timestamp:array<string>,forecast:array<double>>')\n",
    "  def forecast_udf(iterator: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.DataFrame]:\n",
    "    \n",
    "    ## Initialization step\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from nixtla import NixtlaClient  # Import NixtlaClient from the nixtla library\n",
    "\n",
    "    # Initialize the NixtlaClient with the provided model URL and API key\n",
    "    model = NixtlaClient(\n",
    "      base_url=model_url,\n",
    "      api_key=api_key)\n",
    "\n",
    "    ## Inference step\n",
    "    for timeseries, past_values in iterator:\n",
    "      median = []  # Initialize a list to store the forecast results\n",
    "      for ts, y in zip(timeseries, past_values):\n",
    "        # Create a DataFrame from the time series and past values\n",
    "        tdf = pd.DataFrame({\"timestamp\": ts,\n",
    "                            \"value\": y})\n",
    "        # Generate a forecast using the NixtlaClient model\n",
    "        pred = model.forecast(\n",
    "                      df=tdf,\n",
    "                      h=prediction_length,\n",
    "                      time_col=\"timestamp\",\n",
    "                      target_col=\"value\")\n",
    "        \n",
    "        # Append the forecast results to the median list\n",
    "        median.append({'timestamp': list(pred['timestamp'].astype(str).values),\n",
    "                       'forecast': list(pred['TimeGPT'].values)})\n",
    "    # Yield the results as a pandas DataFrame\n",
    "    yield pd.DataFrame(median)  \n",
    "    \n",
    "  return forecast_udf  # Return the forecast UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bb67bfc-9a02-44ed-829d-03173f8bd827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We specify the requirements of our forecasts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c0f566-6dd9-40e9-98e3-645c0c2a73e5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Forecasting with TimeGEN on Azure AI"
    }
   },
   "outputs": [],
   "source": [
    "prediction_length = 12  # Time horizon for forecasting\n",
    "api_key = dbutils.secrets.get(scope=scope_name, key=key_name) # Get credential from secrets \n",
    "freq = \"D\" # Frequency of the time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7977598-3955-41c9-80c9-989d1c6442a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's generate the forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66fe1624-92bb-463c-9b95-a1ac5a9d7bda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Pandas UDF\n",
    "forecast_udf = create_forecast_udf(\n",
    "  model_url=model_url, \n",
    "  api_key=api_key,\n",
    "  )\n",
    "\n",
    "# Apply Pandas UDF to the dataframe\n",
    "forecasts = df.select(\n",
    "  df.unique_id,\n",
    "  forecast_udf(\"ds\", \"y\").alias(\"forecast\"),\n",
    "  ).select(\"unique_id\", \"forecast.timestamp\", \"forecast.forecast\")\n",
    "\n",
    "display(forecasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf1b20a8-c0c0-4f71-8300-5bf53f1c530e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Register Model\n",
    "We will package our model using [`mlflow.pyfunc.PythonModel`](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html) and register this in Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdc97ea2-028f-4dcb-957a-7bc8c926d4e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "import numpy as np\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import DataType, Schema, TensorSpec, ColSpec, ParamSpec, ParamSchema\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")  # Set the MLflow registry URI to Databricks Unity Catalog.\n",
    "experiment_name = \"/Shared/timegpt/\"\n",
    "\n",
    "# Define a custom MLflow Python model class for TimeGPTPipeline\n",
    "class TimeGPTPipeline(mlflow.pyfunc.PythonModel):\n",
    "  def __init__(self, model_url, api_key):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from nixtla import NixtlaClient  # Import NixtlaClient from the nixtla library\n",
    "    self.model_url = model_url  # Store the model URL\n",
    "    self.api_key = api_key  # Store the API key\n",
    "  \n",
    "  def predict(self, context, input_data, params=None):\n",
    "    from nixtla import NixtlaClient  # Import NixtlaClient from the nixtla library\n",
    "    model = NixtlaClient(\n",
    "        base_url=self.model_url,\n",
    "        api_key=self.api_key)  # Initialize the NixtlaClient with the stored model URL and API key\n",
    "    \n",
    "    # Generate a forecast using the NixtlaClient model\n",
    "    pred = model.forecast(\n",
    "              df=input_data,\n",
    "              h=params['h'],  # Use the horizon length from the params\n",
    "              time_col=\"timestamp\",\n",
    "              target_col=\"value\")\n",
    "    # Rename the forecast column to 'forecast'\n",
    "    pred.rename(columns={'TimeGPT': 'forecast'},\n",
    "                inplace=True)\n",
    "    return pred  # Return the prediction DataFrame\n",
    "\n",
    "# Initialize the custom TimeGPTPipeline with the specified model URL and API key\n",
    "pipeline = TimeGPTPipeline(model_url=model_url, api_key=api_key)\n",
    "\n",
    "# Define the input and output schema for the model\n",
    "input_schema = Schema([ColSpec.from_json_dict(**{\"type\": \"datetime\", \"name\": \"timestamp\", \"required\": True}),\n",
    "                       ColSpec.from_json_dict(**{\"type\": \"double\", \"name\": \"value\", \"required\": True})])\n",
    "output_schema = Schema([ColSpec.from_json_dict(**{\"type\": \"datetime\", \"name\": \"timestamp\", \"required\": True}),\n",
    "                       ColSpec.from_json_dict(**{\"type\": \"double\", \"name\": \"forecast\", \"required\": True})])\n",
    "param_schema = ParamSchema([ParamSpec.from_json_dict(**{\"type\": \"integer\", \"name\": \"h\", \"default\": 12})])\n",
    "# Create a ModelSignature object to represent the input, output, and parameter schema\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema, params=param_schema)\n",
    "\n",
    "# Define the registered model name using variables for catalog and database\n",
    "registered_model_name = f\"{catalog}.{db}.timegpt\"\n",
    "\n",
    "# Filter the DataFrame to get records with the specified unique_id and convert to a pandas DataFrame\n",
    "pdf = df.filter(df.unique_id == 'D7').toPandas()\n",
    "pdf = {\n",
    "  \"timestamp\" : list(pdf['ds'][0]),\n",
    "  \"value\" : list(pdf['y'][0])\n",
    "}\n",
    "# Get a single time series from the dataset\n",
    "pdf = get_single_time_series('D4')\n",
    "\n",
    "# set current experiment\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Log and register the model with MLflow\n",
    "with mlflow.start_run() as run:\n",
    "  mlflow.pyfunc.log_model(\n",
    "    \"model\",  # The artifact path where the model is logged\n",
    "    python_model=pipeline,  # The custom Python model to log\n",
    "    registered_model_name=registered_model_name,  # The name to register the model under\n",
    "    signature=signature,  # The model signature\n",
    "    input_example=pdf[:10],  # An example input to log with the model\n",
    "    pip_requirements=[\n",
    "      \"nixtla\"  # Python package requirements\n",
    "    ]\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90cb01ea-112d-4ce9-9e13-d75aa0c3e849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Reload Model\n",
    "Once the registration is complete, we will reload the model and generate forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "926209e2-adeb-4e5f-9288-58231a1e4a82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "mlflow_client = MlflowClient()\n",
    "\n",
    "# Define a function to get the latest version number of a registered model\n",
    "def get_latest_model_version(mlflow_client, registered_model_name):\n",
    "    latest_version = 1  # Initialize the latest version number to 1\n",
    "    # Iterate through all model versions of the specified registered model\n",
    "    for mv in mlflow_client.search_model_versions(f\"name='{registered_model_name}'\"):\n",
    "        version_int = int(mv.version)  # Convert the version number to an integer\n",
    "        if version_int > latest_version:  # Check if the current version is greater than the latest version\n",
    "            latest_version = version_int  # Update the latest version number\n",
    "    return latest_version  # Return the latest version number\n",
    "\n",
    "# Get the latest version number of the specified registered model\n",
    "model_version = get_latest_model_version(mlflow_client, registered_model_name)\n",
    "# Construct the model URI using the registered model name and the latest version number\n",
    "logged_model = f\"models:/{registered_model_name}/{model_version}\"\n",
    "\n",
    "# Load the model as a PyFuncModel using the constructed model URI\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41617f21-4004-4e21-9d2c-6f32535a0fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Test the endpoint before deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3907f083-5308-4d85-82e0-65a61bc4bc86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get random data points\n",
    "pdf = get_single_time_series('D4')\n",
    "\n",
    "# Generate forecasts\n",
    "loaded_model.predict(pdf, params = {'h': 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "359e33c5-e19e-4eae-914e-6bf44d6e5ff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy Model\n",
    "We will deploy our model behind a real-time endpoint of [Databricks Mosaic AI Model Serving](https://www.databricks.com/product/model-serving)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bce31eaa-a808-4649-b4be-e9c00d21f892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# With the token, you can create our authorization header for our subsequent REST calls\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\n",
    "headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Next you need an endpoint at which to execute your request which you can get from the notebook's tags collection\n",
    "java_tags = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags()\n",
    "\n",
    "# This object comes from the Java CM - Convert the Java Map opject to a Python dictionary\n",
    "tags = sc._jvm.scala.collection.JavaConversions.mapAsJavaMap(java_tags)\n",
    "\n",
    "# Lastly, extract the Databricks instance (domain name) from the dictionary\n",
    "instance = tags[\"browserHostName\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdaf554c-d244-4a2a-8ed4-de96bc367c91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "model_serving_endpoint_name = \"timegpt\"\n",
    "\n",
    "my_json = {\n",
    "    \"name\": model_serving_endpoint_name,\n",
    "    \"config\": {\n",
    "        \"served_models\": [\n",
    "            {\n",
    "                \"model_name\": registered_model_name,\n",
    "                \"model_version\": model_version,\n",
    "                \"workload_type\": \"CPU_SMALL\",\n",
    "                \"workload_size\": \"Small\",\n",
    "                \"scale_to_zero_enabled\": \"true\",\n",
    "            }\n",
    "        ],\n",
    "        \"auto_capture_config\": {\n",
    "            \"catalog_name\": catalog,\n",
    "            \"schema_name\": db,\n",
    "            \"table_name_prefix\": model_serving_endpoint_name,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Make sure to drop the inference table of it exists\n",
    "_ = spark.sql(\n",
    "    f\"DROP TABLE IF EXISTS {catalog}.{db}.`{model_serving_endpoint_name}_payload`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0595a2b2-5554-4d78-89a1-74a65f455bd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to create an endpoint in Model Serving and deploy the model behind it\n",
    "def func_create_endpoint(model_serving_endpoint_name):\n",
    "    # get endpoint status\n",
    "    endpoint_url = f\"https://{instance}/api/2.0/serving-endpoints\"\n",
    "    url = f\"{endpoint_url}/{model_serving_endpoint_name}\"\n",
    "    r = requests.get(url, headers=headers)\n",
    "    if \"RESOURCE_DOES_NOT_EXIST\" in r.text:\n",
    "        print(\n",
    "            \"Creating this new endpoint: \",\n",
    "            f\"https://{instance}/serving-endpoints/{model_serving_endpoint_name}/invocations\",\n",
    "        )\n",
    "        re = requests.post(endpoint_url, headers=headers, json=my_json)\n",
    "    else:\n",
    "        new_model_version = (my_json[\"config\"])[\"served_models\"][0][\"model_version\"]\n",
    "        print(\n",
    "            \"This endpoint existed previously! We are updating it to a new config with new model version: \",\n",
    "            new_model_version,\n",
    "        )\n",
    "        # update config\n",
    "        url = f\"{endpoint_url}/{model_serving_endpoint_name}/config\"\n",
    "        re = requests.put(url, headers=headers, json=my_json[\"config\"])\n",
    "        # wait till new config file in place\n",
    "        import time, json\n",
    "\n",
    "        # get endpoint status\n",
    "        url = f\"https://{instance}/api/2.0/serving-endpoints/{model_serving_endpoint_name}\"\n",
    "        retry = True\n",
    "        total_wait = 0\n",
    "        while retry:\n",
    "            r = requests.get(url, headers=headers)\n",
    "            assert (\n",
    "                r.status_code == 200\n",
    "            ), f\"Expected an HTTP 200 response when accessing endpoint info, received {r.status_code}\"\n",
    "            endpoint = json.loads(r.text)\n",
    "            if \"pending_config\" in endpoint.keys():\n",
    "                seconds = 10\n",
    "                print(\"New config still pending\")\n",
    "                if total_wait < 6000:\n",
    "                    # if less the 10 mins waiting, keep waiting\n",
    "                    print(f\"Wait for {seconds} seconds\")\n",
    "                    print(f\"Total waiting time so far: {total_wait} seconds\")\n",
    "                    time.sleep(10)\n",
    "                    total_wait += seconds\n",
    "                else:\n",
    "                    print(f\"Stopping,  waited for {total_wait} seconds\")\n",
    "                    retry = False\n",
    "            else:\n",
    "                print(\"New config in place now!\")\n",
    "                retry = False\n",
    "\n",
    "    assert (\n",
    "        re.status_code == 200\n",
    "    ), f\"Expected an HTTP 200 response, received {re.status_code}\"\n",
    "\n",
    "# Function to delete the endpoint from Model Serving\n",
    "def func_delete_model_serving_endpoint(model_serving_endpoint_name):\n",
    "    endpoint_url = f\"https://{instance}/api/2.0/serving-endpoints\"\n",
    "    url = f\"{endpoint_url}/{model_serving_endpoint_name}\"\n",
    "    response = requests.delete(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            f\"Request failed with status {response.status_code}, {response.text}\"\n",
    "        )\n",
    "    else:\n",
    "        print(model_serving_endpoint_name, \"endpoint is deleted!\")\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90f9f046-e152-4446-bd5d-e4e71d213f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "func_create_endpoint(model_serving_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee0b4e56-b3d8-4945-b1b6-2bdc8c1d6a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import mlflow\n",
    "\n",
    "# Define a function to wait for a serving endpoint to be ready\n",
    "def wait_for_endpoint():\n",
    "    endpoint_url = f\"https://{instance}/api/2.0/serving-endpoints\"  # Construct the base URL for the serving endpoints API\n",
    "    while True:  # Infinite loop to repeatedly check the status of the endpoint\n",
    "        url = f\"{endpoint_url}/{model_serving_endpoint_name}\"  # Construct the URL for the specific model serving endpoint\n",
    "        response = requests.get(url, headers=headers)  # Send a GET request to the endpoint URL with the necessary headers\n",
    "        \n",
    "        # Ensure the response status code is 200 (OK)\n",
    "        assert (\n",
    "            response.status_code == 200\n",
    "        ), f\"Expected an HTTP 200 response, received {response.status_code}\\n{response.text}\"\n",
    "\n",
    "        # Extract the status of the endpoint from the response JSON\n",
    "        status = response.json().get(\"state\", {}).get(\"ready\", {})\n",
    "        # print(\"status\",status)  # Optional: Print the status for debugging purposes\n",
    "        \n",
    "        # Check if the endpoint status is \"READY\"\n",
    "        if status == \"READY\":\n",
    "            print(status)  # Print the status if the endpoint is ready\n",
    "            print(\"-\" * 80)  # Print a separator line for clarity\n",
    "            return  # Exit the function when the endpoint is ready\n",
    "        else:\n",
    "            # Print a message indicating the endpoint is not ready and wait for 5 minutes\n",
    "            print(f\"Endpoint not ready ({status}), waiting 5 minutes\")\n",
    "            time.sleep(300)  # Wait for 300 seconds (5 minutes) before checking again\n",
    "\n",
    "# Get the Databricks web application URL using an MLflow utility function\n",
    "api_url = mlflow.utils.databricks_utils.get_webapp_url()\n",
    "\n",
    "# Call the wait_for_endpoint function to wait for the serving endpoint to be ready\n",
    "wait_for_endpoint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a80074-59b7-454f-90b5-00b6560246a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Online Forecast\n",
    "Once the endpoint is ready, let's send a request to the model and generate an online forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffce19e0-94ad-4e1f-8d5f-faae5f7f4f9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace URL with the end point invocation url you get from Model Seriving page.\n",
    "endpoint_url = f\"https://{instance}/serving-endpoints/{model_serving_endpoint_name}/invocations\"\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "def forecast(input_data, url=endpoint_url, databricks_token=token):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {databricks_token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    body = {'dataframe_split': input_data.to_dict(orient='split'),\"params\" :{'h':20}}\n",
    "    data = json.dumps(body)\n",
    "    response = requests.request(method=\"POST\", headers=headers, url=url, data=data)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            f\"Request failed with status {response.status_code}, {response.text}\"\n",
    "        )\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50ad13e1-4f48-42b7-965a-95e04f6e60c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test online forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e52dc61-a074-4695-9106-23b2f5e27cce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Send request to the endpoint\n",
    "pdf = get_single_time_series('D3')\n",
    "pdf['timestamp'] = pdf['timestamp'].astype(str)\n",
    "forecast(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42ae7d54-ab56-4b75-b203-1a4ca3f64eec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delete the serving endpoint\n",
    "func_delete_model_serving_endpoint(model_serving_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf418d8f-0950-409f-8c1e-1e56e8acc3eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "© 2024 Databricks, Inc. All rights reserved. \n",
    "\n",
    "The sources in all notebooks in this directory and the sub-directories are provided subject to the Databricks License. All included or referenced third party libraries are subject to the licenses set forth below."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "04a_timegpt_load_inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
