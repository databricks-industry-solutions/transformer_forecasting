{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cbf7267-6af6-4a4c-87ab-3c59d9c7d4c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This is an example notebook that shows how to use [Moirai](https://github.com/SalesforceAIResearch/uni2ts) models on Databricks. The notebook loads, fine-tunes, and registers the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74af0073-d0dc-4551-8b31-7676c0d6b537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cluster setup\n",
    "We recommend using a cluster with [Databricks Runtime 16.4 LTS for ML](https://docs.databricks.com/en/release-notes/runtime/16.4lts-ml.html). The cluster can be single-node or multi-node with one or more GPU instances on each worker: e.g. [g5.12xlarge [A10G]](https://aws.amazon.com/ec2/instance-types/g5/) on AWS or [Standard_NV72ads_A10_v5](https://learn.microsoft.com/en-us/azure/virtual-machines/nva10v5-series) on Azure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ac9bff6-b67b-485d-a4ab-4b4b300ede8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Install package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aa4f679-8da5-4037-8faa-aa5ed9a31d1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install uni2ts==1.2.0 --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c48d6c20-c4e8-47c1-92a1-8807b557f5e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare data \n",
    "We use [`datasetsforecast`](https://github.com/Nixtla/datasetsforecast/tree/main/) package to download M4 data. M4 dataset contains a set of time series which we use for testing. See the `data_preparation` notebook for a number of custom functions we wrote to convert M4 time series to an expected format.\n",
    "\n",
    "Make sure that the catalog and the schema already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93e15bfc-f5b2-4a5b-90af-e37efb75476b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"tsfm\"  # Name of the catalog we use to manage our assets\n",
    "db = \"random\"  # Name of the schema we use to manage our assets (e.g. datasets)\n",
    "volume = \"moirai_fine_tune\" # Name of the volume we store the data and the weigts\n",
    "model = \"moirai-1.1-R-small\"  # Alternatibely: moirai-1.0-R-base, moirai-1.0-R-large\n",
    "n = 100  # Number of time series to sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb716876-a0ab-4fc1-967b-cbe36e367aea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make sure that the database exists.\n",
    "_ = spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{db}\")\n",
    "\n",
    "# Make sure that the volume exists. We stored the fine-tuned weights here.\n",
    "_ = spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog}.{db}.{volume}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d42c1bb-e1f3-4c30-900f-c3b4f96f205a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We synthesize `n` number of time series (randomly sampled) at daily resolution and store it as a csv file in UC Volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb286f1e-93a5-422f-a775-36a6ad766692",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_dict = {}\n",
    "\n",
    "for i in range(n):\n",
    "\n",
    "    # Create a date range for the index\n",
    "    date_range = pd.date_range(start='2021-01-01', end='2023-12-31', freq='D')\n",
    "\n",
    "    # Create a DataFrame with a date range index and two columns: 'item_id' and 'target'\n",
    "    df = pd.DataFrame({\n",
    "        'item_id': str(f\"item_{i}\"),\n",
    "        'target': np.random.randn(len(date_range))\n",
    "    }, index=date_range)\n",
    "\n",
    "    # Set 'item_id' as the second level of the MultiIndex\n",
    "    df.set_index('item_id', append=True, inplace=True)\n",
    "\n",
    "    # Sort the index\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    df_dict[i] = df\n",
    "\n",
    "\n",
    "pdf = pd.concat([df_dict[i] for i in range(n)])\n",
    "pdf.to_csv(f\"/Volumes/{catalog}/{db}/{volume}/random.csv\", index=True)\n",
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a4d631a-19aa-449a-9c90-fc9ccbff05ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This dotenv file is needed to call the [`uni2ts.data.builder.simple`](https://github.com/SalesforceAIResearch/uni2ts/blob/main/src/uni2ts/data/builder/simple.py) function from the [`uni2ts`](https://github.com/SalesforceAIResearch/uni2ts) library to build a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae431324-f103-471f-8dbc-fb8582fdf307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import site\n",
    "\n",
    "# Construct the path to the 'uni2ts' directory within the site-packages directory.\n",
    "# site.getsitepackages()[0] returns the path to the first directory in the list of site-packages directories.\n",
    "uni2ts = os.path.join(site.getsitepackages()[0], \"uni2ts\")\n",
    "\n",
    "# Construct the path to the '.env' file within the 'uni2ts' directory.\n",
    "dotenv = os.path.join(uni2ts, \".env\")\n",
    "\n",
    "# Set the 'DOTENV' environment variable to the path of the '.env' file.\n",
    "# This tells the system where to find the '.env' file.\n",
    "os.environ['DOTENV'] = dotenv\n",
    "\n",
    "# Set the 'CUSTOM_DATA_PATH' environment variable to a path constructed using the provided 'catalog', 'db', and 'volume'.\n",
    "# This sets a custom data path for the application to use.\n",
    "os.environ['CUSTOM_DATA_PATH'] = f\"/Volumes/{catalog}/{db}/{volume}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d34324e2-dc74-49c0-bc65-729b4052330e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "rm -f $DOTENV\n",
    "touch $DOTENV\n",
    "echo \"CUSTOM_DATA_PATH=$CUSTOM_DATA_PATH\" >> $DOTENV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30185a41-c467-4421-9883-1e006986e402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We convert the dataset into the Uni2TS format. `random` is the name we give to the training dataset, which we load from our volume's location. See the [README](https://github.com/SalesforceAIResearch/uni2ts/tree/main?tab=readme-ov-file#fine-tuning) of Uni2TS for more information on the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65d13bad-0480-41c8-82a9-ac7d2e1850b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh python -m uni2ts.data.builder.simple random /Volumes/tsfm/random/moirai_fine_tune/random.csv \\\n",
    "    --dataset_type long \\\n",
    "    --offset 640"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f860bfd1-4e3d-4b63-9d8b-0b0015a7b5f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Run Fine-tuning\n",
    "\n",
    "In this example, we wil fine-tune `moirai-1.0-R-small` for max 100 epochs with early stopping (can be specified here: `moirai/conf/finetune/default.yaml`). The learning rate is set to 1e-3, which you can modify in the model specific configuration file: `moirai/conf/finetune/model/moirai_1.0_R_small.yaml`. \n",
    "\n",
    "Make sure that you have the configuration yaml files placed inside the `moirai/conf` folder and the `moirai/train.py` script in the same directory. These two assets are taken directly from and [cli/conf](https://github.com/SalesforceAIResearch/uni2ts/tree/main/cli/conf) and [cli/train.py](https://github.com/SalesforceAIResearch/uni2ts/blob/main/cli/train.py). They are subject to change as the Moirai' team develops the framework further. Keep your eyes on the latest changes (we will try too) and use the latest versions as needed.\n",
    "\n",
    "The key configuration files to be customized for you use case are `moirai/conf/finetune/default.yaml`, `moirai/conf/finetune/data/random.yaml` and `moirai/conf/finetune/val_data/random.yaml`. Read through the Moirai [documentation](https://github.com/SalesforceAIResearch/uni2ts) for more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9185b3df-157f-474d-9cb4-752e971b25ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh python moirai_train.py \\\n",
    "  -cp configs/moirai \\\n",
    "  run_name=random_run \\\n",
    "  model=moirai_1.1_R_small \\\n",
    "  data=random \\\n",
    "  val_data=random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a60dafe3-7e3c-4b60-9fd0-04382f0eebd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Register Model\n",
    "We get the fine-tuned weights from the run from the UC volume, wrap the pipeline with [`mlflow.pyfunc.PythonModel`](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html) and register this on Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f2a390a-185b-484c-9b49-6b6b5a6f869d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "import numpy as np\n",
    "from mlflow.models.signature import ModelSignature  # Used to define the model input and output schema.\n",
    "from mlflow.types import DataType, Schema, TensorSpec  # Used to define the data types and structure for model inputs and outputs.\n",
    "\n",
    "# Set the MLflow registry URI to Databricks Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "experiment_name = \"/Shared/moirai/\"\n",
    "\n",
    "# Define a custom MLflow Python model class\n",
    "class FineTunedMoiraiModel(mlflow.pyfunc.PythonModel):  \n",
    "    def predict(self, context, input_data, params=None):\n",
    "        from einops import rearrange  # Einops is a library for tensor operations.\n",
    "        from uni2ts.model.moirai import MoiraiForecast, MoiraiModule  # Import the required classes from the Moirai model.\n",
    "        \n",
    "        # Determine the device to run the model on (GPU if available, otherwise CPU)\n",
    "        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        \n",
    "        # Load the pre-trained Moirai model from the checkpoint\n",
    "        model = MoiraiForecast.load_from_checkpoint(\n",
    "            prediction_length=10,\n",
    "            context_length=len(input_data),\n",
    "            patch_size=32,\n",
    "            num_samples=10,\n",
    "            target_dim=1,\n",
    "            feat_dynamic_real_dim=0,\n",
    "            past_feat_dynamic_real_dim=0,\n",
    "            checkpoint_path=context.artifacts[\"weights\"],\n",
    "        ).to(device)\n",
    "        \n",
    "        # Prepare the input data for the model\n",
    "        # Time series values. Shape: (batch, time, variate)\n",
    "        past_target = rearrange(\n",
    "            torch.as_tensor(input_data, dtype=torch.float32), \"t -> 1 t 1\"\n",
    "        )\n",
    "        # 1s if the value is observed, 0s otherwise. Shape: (batch, time, variate)\n",
    "        past_observed_target = torch.ones_like(past_target, dtype=torch.bool)\n",
    "        # 1s if the value is padding, 0s otherwise. Shape: (batch, time)\n",
    "        past_is_pad = torch.zeros_like(past_target, dtype=torch.bool).squeeze(-1)\n",
    "        \n",
    "        # Generate the forecast using the model\n",
    "        forecast = model(\n",
    "            past_target=past_target.to(device),\n",
    "            past_observed_target=past_observed_target.to(device),\n",
    "            past_is_pad=past_is_pad.to(device),\n",
    "        )\n",
    "        \n",
    "        # Return the median forecast\n",
    "        return np.median(forecast.cpu()[0], axis=0)\n",
    "\n",
    "# Define the input schema for the model\n",
    "input_schema = Schema([TensorSpec(np.dtype(np.double), (-1,))])\n",
    "# Define the output schema for the model\n",
    "output_schema = Schema([TensorSpec(np.dtype(np.uint8), (-1,))])\n",
    "# Create a ModelSignature object to represent the input and output schema\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "# Create an example input to log with the model\n",
    "input_example = np.random.rand(52)\n",
    "\n",
    "# Define the registered model name using variables for catalog, database, and volume\n",
    "registered_model_name = f\"{catalog}.{db}.moirai-1-1-r-small_finetuned\"\n",
    "\n",
    "# Define the path to the model weights\n",
    "weights = f\"/Volumes/{catalog}/{db}/{volume}/outputs/moirai_1.1_R_small/random/random_run/checkpoints/epoch=0-step=100.ckpt\"\n",
    "\n",
    "# set current experiment\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Log and register the model with MLflow\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.pyfunc.log_model(\n",
    "        \"model\",  # The artifact path where the model is logged\n",
    "        python_model=FineTunedMoiraiModel(),  # The custom Python model to log\n",
    "        registered_model_name=registered_model_name,  # The name to register the model under\n",
    "        artifacts={\"weights\": weights},  # The model artifacts to log\n",
    "        signature=signature,  # The model signature\n",
    "        input_example=input_example,  # An example input to log with the model\n",
    "        pip_requirements=[\n",
    "            \"uni2ts\",\n",
    "        ],  # The Python packages required to run the model\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6b209fd-254c-4faa-b574-be78f8a25bfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Reload Model\n",
    "We reload the model from the registry and perform forecasting on a randomly generated time series (for testing purpose). You can also go ahead and deploy this model behind a Model Serving's real-time endpoint. See the previous notebook: [`01_moirai_load_inference`](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/examples/foundation-model-examples/chronos/02_moirai_load_inference.py) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1edc1701-451e-430a-96db-675931271a61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "# Function to get the latest version number of a registered model\n",
    "def get_latest_model_version(client, registered_model_name):\n",
    "    latest_version = 1  # Initialize the latest version number to 1\n",
    "    # Iterate through all model versions of the specified registered model\n",
    "    for mv in client.search_model_versions(f\"name='{registered_model_name}'\"):\n",
    "        version_int = int(mv.version)  # Convert the version number to an integer\n",
    "        if version_int > latest_version:  # Check if the current version is greater than the latest version\n",
    "            latest_version = version_int  # Update the latest version number\n",
    "    return latest_version  # Return the latest version number\n",
    "\n",
    "# Get the latest version number of the specified registered model\n",
    "model_version = get_latest_model_version(client, registered_model_name)\n",
    "# Construct the model URI using the registered model name and the latest version number\n",
    "logged_model = f\"models:/{registered_model_name}/{model_version}\"\n",
    "\n",
    "# Load the model as a PyFuncModel\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "# Create input data for the model\n",
    "input_data = np.random.rand(52)  # Generate random input data of shape (52,)\n",
    "\n",
    "# Generate forecasts using the loaded model\n",
    "loaded_model.predict(input_data)  # Use the loaded model to make predictions on the input data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d149d7ab-245c-49ac-b41b-e3ef31b224c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "© 2024 Databricks, Inc. All rights reserved. \n",
    "\n",
    "The sources in all notebooks in this directory and the sub-directories are provided subject to the Databricks License. All included or referenced third party libraries are subject to the licenses set forth below."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2038282820364573,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "02b_moirai_fine_tune",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
